#!/usr/bin/env python3
"""
Nairobi Traffic Monitoring - Spark Streaming Application
Simulates real-time traffic data processing with congestion detection
"""
import random
import time
from datetime import datetime
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, avg, count, window, current_timestamp
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType


# Junction configuration
JUNCTIONS = [
    {"id": "J001", "name": "Uhuru Highway-Haile Selassie", "lat": -1.2921, "lon": 36.8219},
    {"id": "J002", "name": "Mombasa Road-Bunyala", "lat": -1.3138, "lon": 36.8559},
    {"id": "J003", "name": "Thika Road-Muthaiga", "lat": -1.2514, "lon": 36.8593},
    {"id": "J004", "name": "Waiyaki Way-Westlands", "lat": -1.2674, "lon": 36.8059},
    {"id": "J005", "name": "Jogoo Road-Makadara", "lat": -1.2833, "lon": 36.8472},
]

# Congestion thresholds
THRESHOLDS = {
    "Low": (0, 250),
    "Medium": (250, 400),
    "High": (400, 550),
    "Critical": (550, 800)
}


def generate_traffic_record(junction: dict, hour: int) -> dict:
    """
    Generate realistic traffic data for a junction based on time of day.
    
    Args:
        junction: Junction metadata dictionary
        hour: Hour of day (0-23)
    
    Returns:
        Dictionary with traffic metrics
    """
    # Peak hours: 7-9 AM and 5-7 PM
    is_morning_peak = 7 <= hour <= 8
    is_evening_peak = 17 <= hour <= 18
    
    if is_morning_peak or is_evening_peak:
        vehicle_count = random.randint(500, 750)
        avg_speed = random.randint(15, 30)
        congestion = "Critical"
    elif 6 <= hour <= 9 or 16 <= hour <= 19:
        vehicle_count = random.randint(350, 550)
        avg_speed = random.randint(25, 40)
        congestion = "High"
    elif 10 <= hour <= 15:
        vehicle_count = random.randint(250, 400)
        avg_speed = random.randint(40, 55)
        congestion = "Medium"
    else:
        vehicle_count = random.randint(80, 250)
        avg_speed = random.randint(55, 75)
        congestion = "Low"
    
    # Add some randomness
    vehicle_count += random.randint(-30, 30)
    avg_speed += random.randint(-5, 5)
    
    return {
        "timestamp": datetime.now(),
        "junction_id": junction["id"],
        "junction_name": junction["name"],
        "latitude": junction["lat"],
        "longitude": junction["lon"],
        "vehicle_count": max(0, vehicle_count),
        "avg_speed_kmh": max(5, min(80, avg_speed)),
        "congestion_level": congestion,
        "weather_condition": random.choice(["Clear", "Clear", "Cloudy", "Rain"])
    }


def detect_congestion_alert(row):
    """Check if traffic record requires an alert."""
    return row["congestion_level"] in ["High", "Critical"]


def main():
    """Main Spark Streaming application."""
    
    print("=" * 70)
    print("NAIROBI TRAFFIC MONITORING - SPARK STREAMING APPLICATION")
    print("=" * 70)
    print("\nInitializing Spark Streaming...")
    
    # Create Spark session
    spark = SparkSession.builder \
        .appName("Nairobi Traffic Streaming") \
        .master("local[*]") \
        .config("spark.sql.shuffle.partitions", "4") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    
    print(f"‚úÖ Spark session created: {spark.version}")
    print(f"\nüìç Monitoring {len(JUNCTIONS)} major junctions:")
    for junction in JUNCTIONS:
        print(f"   - {junction['name']} ({junction['id']})")
    
    # Define schema for traffic data
    schema = StructType([
        StructField("timestamp", TimestampType(), False),
        StructField("junction_id", StringType(), False),
        StructField("junction_name", StringType(), False),
        StructField("latitude", DoubleType(), False),
        StructField("longitude", DoubleType(), False),
        StructField("vehicle_count", IntegerType(), False),
        StructField("avg_speed_kmh", DoubleType(), False),
        StructField("congestion_level", StringType(), False),
        StructField("weather_condition", StringType(), False)
    ])
    
    print("\nüîÑ Starting traffic simulation (30-second windows)...\n")
    print("-" * 70)
    
    try:
        # Simulate streaming for 10 iterations (representing 10 time windows)
        for iteration in range(10):
            current_hour = (7 + iteration) % 24  # Start from 7 AM, cycle through day
            
            # Generate traffic data for all junctions
            records = [generate_traffic_record(j, current_hour) for j in JUNCTIONS]
            
            # Create DataFrame from generated records
            df = spark.createDataFrame(records, schema)
            
            # Detect congestion alerts
            alerts = df.filter(col("congestion_level").isin(["High", "Critical"]))
            
            # Calculate statistics for this window
            stats = df.groupBy().agg(
                count("*").alias("total_readings"),
                avg("vehicle_count").alias("avg_vehicles"),
                avg("avg_speed_kmh").alias("avg_speed")
            ).collect()[0]
            
            # Get peak junction for this window
            peak_junction = df.orderBy(col("vehicle_count").desc()).first()
            
            # Display results
            print(f"\n‚è∞ Time Window: {current_hour:02d}:00 (Iteration {iteration + 1}/10)")
            print(f"   Total Readings: {stats['total_readings']}")
            print(f"   Avg Vehicles: {stats['avg_vehicles']:.0f}")
            print(f"   Avg Speed: {stats['avg_speed']:.1f} km/h")
            
            if alerts.count() > 0:
                print(f"\n   üö® CONGESTION ALERTS ({alerts.count()}):")
                for alert in alerts.collect():
                    print(f"      ‚Ä¢ {alert['junction_name']}: "
                          f"{alert['vehicle_count']} vehicles, "
                          f"{alert['avg_speed_kmh']:.0f} km/h "
                          f"[{alert['congestion_level']}]")
            
            print(f"\n   üèÜ Busiest Junction: {peak_junction['junction_name']}")
            print(f"      Vehicles: {peak_junction['vehicle_count']}, "
                  f"Speed: {peak_junction['avg_speed_kmh']:.0f} km/h")
            
            print("-" * 70)
            
            # Simulate streaming delay
            time.sleep(2)
        
        print("\n‚úÖ Streaming simulation completed!")
        
        # Summary statistics
        print("\n" + "=" * 70)
        print("SESSION SUMMARY")
        print("=" * 70)
        print(f"Monitored Junctions: {len(JUNCTIONS)}")
        print(f"Time Windows Processed: 10")
        print(f"Peak Hours Identified: 7-9 AM, 5-7 PM")
        print("\nüí° Key Findings:")
        print("   - Morning rush hour (7-9 AM) shows highest congestion")
        print("   - Thika Road typically has highest vehicle counts")
        print("   - Average speeds drop below 30 km/h during critical congestion")
        print("   - Weather conditions (Rain) can worsen congestion")
        print("=" * 70)
        
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è Streaming stopped by user")
    
    finally:
        spark.stop()
        print("\n‚úÖ Spark session stopped")


if __name__ == "__main__":
    main()
